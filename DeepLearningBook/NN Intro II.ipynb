{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning. Chapter 2: Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"secContent\">Content</span>\n",
    "\n",
    "1. [Summary of Chap1's code]()\n",
    "2. [The Training: First Learning Step](#secTheTraining)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Summary of Chap1\n",
    "#\n",
    "#\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "def synapticPotentialWeighted(X,W,b=0):\n",
    "    return np.dot(X,W[:len(X)])+b\n",
    "    \n",
    "def synapticPotentialAverage(X,W=None):\n",
    "    return sum(X)/len(X)\n",
    "    \n",
    "def synapticPotentialMedian(X,W=None):\n",
    "    \"\"\"\n",
    "    To make sense, \n",
    "    WT := \\sum |W_i| := total # of X values (not all distinct) seen.\n",
    "    Assing each X_i to its weight W_i -> (X_i,W_i)\n",
    "    sort by X_i\n",
    "    lx2=floor(WT/2)\n",
    "    if exists j ; |W_j| = lx2 :\n",
    "        md = ( W_j*X_j + W_(j+1) *X_(j+1) ) / ( W_j + W_(j+1)) #weighted average of both points\n",
    "    if WT%2==0: \n",
    "    else:\n",
    "        md\n",
    "        \n",
    "    Example: w1=7 w2=5 w3=2 => WT=14 => lx2=7 => md = (w1*x1+w2*x2 ) / (w1+w2) = (7x1+5x2)/12\n",
    "                  \n",
    "    if lx%2 == 0 ):\n",
    "        md=(Z[lx2-1] + Z[lx2] )*0.5\n",
    "    else:\n",
    "        md=Z[lx2]*1.0 #so that it always returns a float\n",
    "    \"\"\"\n",
    "    \n",
    "    # W=[1,1,...,1] as many 1's as length of X\n",
    "    if W == None: \n",
    "        W=np.ones(len(X)) \n",
    "\n",
    "    #Given array [x1,x2,...xn] -> get the sum_i xi\n",
    "    def foldr(_X,_accu=0):          \n",
    "        if len(_X) == 0: return _accu\n",
    "        if len(_X) == 1: return _accu+_X[0]\n",
    "        return (_accu + _X[-1] + foldr(_X[:-1]) )\n",
    "\n",
    "    WT = foldr( list( map(abs,W) ) )\n",
    "    \n",
    "    def pfilter(par,x):\n",
    "        for a,b in par:\n",
    "            if abs(b) == x: return par.index((a,b))\n",
    "        return None\n",
    "\n",
    "    Z=sorted(list(zip(X,W)))\n",
    "    lx2=floor(WT/2)\n",
    "    #on Z=sorted(X,W): Python will mutate arguments **if it can**\n",
    "               \n",
    "    #if WT%2 == 0:\n",
    "    #    j = pfilter(Z,lx2)\n",
    "    #    try: return (Z[j][1]*Z[j][0]+Z[j+1][1]*Z[j+1][0]) \n",
    "    #    except: print('ERROR j= ',j,\" Z: \",Z, \" WT: \",WT)\n",
    "    #else:\n",
    "    j=0 ; zw=abs(Z[j][1])\n",
    "    while zw < lx2:\n",
    "        j+=1\n",
    "        zw+=abs(Z[j][1]) \n",
    "    if j == len(Z)-1: j-=1\n",
    "    \n",
    "    try: return (Z[j+1][1]*Z[j+1][0]+Z[j][1]*Z[j][0])/(abs(Z[j+1][1])+abs(Z[j][1])) # {wj*xj + w(j+1)*x(j+1)}/(wj+wj+1) \n",
    "    except: print('ERROR j=',j,\" Z: \",Z, \" WT: \",WT)\n",
    "\n",
    "def synapticPotentialMode(X,W):\n",
    "    \"\"\"\n",
    "        To make sense, the weights W should be non-negative.\n",
    "    Choose randomly one among all possible maxima\n",
    "    \n",
    "    When W[1,1] Provides a soft boundary between both phases (classes) with difussion across the boundary\n",
    "    \"\"\"\n",
    "    wmax = max(list(map(abs,W)) )\n",
    "    imax = [ i for i,j in enumerate(list(map(abs,W)) ) if j==wmax ]\n",
    "    #print(\"max weights:\",imax)\n",
    "    i = np.random.choice(imax,1)\n",
    "    return X[i]*W[i]/wmax\n",
    "    \n",
    "##\n",
    "def nonLinearBipolarStep(x,string=None):\n",
    "    if not string: return (-1 if x<0 else 1 )\n",
    "    else: return ('-' if x<0 else '1')\n",
    "\n",
    "def nonLinearHeaviside(x,string=None):\n",
    "    if not string: return (0 if x<0 else 1 )\n",
    "    else: return ('o' if x<0 else '1')\n",
    "\n",
    "def nonLinearRelu(x,string=None):\n",
    "    r = max(0,x)\n",
    "    if not string: return r\n",
    "    else: return str(r)\n",
    "\n",
    "def identity(x,string=None):\n",
    "    if not string: return x\n",
    "    else: return str(x)\n",
    "\n",
    "## General perceptron: called ng_nGenPerceptron in previous Chapter\n",
    "def perceptron(X,W,B=[0],\n",
    "                   synapticPotential=synapticPotentialWeighted,\n",
    "                   #activation=nonLinearBipolarStep,\n",
    "                   activation=nonLinearHeaviside,\n",
    "                   #activation=(lambda z,s: nonLinearBipolarStep(nonLinearRelu(z,None),s)),\n",
    "                   string=True,debug=False,showHidden=False):\n",
    "    if B == None: B=[0]\n",
    "    if debug: print(\"ng_nGenPer: W:\",W,\" X:\",X,\" B:\",B)    \n",
    "    return activation(synapticPotential(X,W,b=B[0]),string=string)\n",
    "\n",
    "## (Feed-forward neural network for XOR function using Bipolar step as activation function\n",
    "##   and weighted sum as aggregator)\n",
    "def nnXOR(X,W,showHidden=None,debug=False):\n",
    "    assert len(W)==6 \n",
    "    strings=[False,False,False,False,False]\n",
    "    h=0\n",
    "    if showHidden : \n",
    "        h=(showHidden-1)%2 # h1-2\n",
    "        assert h>=0\n",
    "        strings[h]=True \n",
    "        \n",
    "    hl=[]\n",
    "    hl.append(perceptron(X=X,W=W[0:2],B=[0],\n",
    "                        synapticPotential=synapticPotentialWeighted,\n",
    "                        activation=nonLinearBipolarStep,\n",
    "                        string=strings[0],debug=debug\n",
    "                        ) \n",
    "             )\n",
    "    hl.append(perceptron(X=X,W=W[2:4],B=[0],\n",
    "                        synapticPotential=synapticPotentialWeighted,\n",
    "                        activation=nonLinearBipolarStep,\n",
    "                        string=strings[1],debug=debug\n",
    "                        ) \n",
    "             )\n",
    "\n",
    "    if showHidden: return hl[h]\n",
    "    \n",
    "    return nPerceptron(X=[hl[0],hl[1]],W=W[4:6],string=True)\n",
    "\n",
    "## AND function\n",
    "def perceptronAND(X):\n",
    "    return perceptron(X=X,W=[1,1],B=[2],\n",
    "                             synapticPotential=synapticPotentialWeighted,\n",
    "                             activation=nonLinearHeaviside,\n",
    "                             string=False,debug=False,showHidden=False)\n",
    "\n",
    "## Neural network for a convex hull pattern\n",
    "def nnConvexHull(X,W,B=[0,0,0,0],\n",
    "                 synapticPotential=synapticPotentialWeighted,\n",
    "                 activation=nonLinearHeaviside,\n",
    "                 string=None,debug=False,showHidden=False):\n",
    "    \"\"\"\n",
    "    Classifies the 2d plane in two distinct regions with an arbitrary polygonal perimetre\n",
    "    \"\"\"\n",
    "    hl=[0,0,0,0]\n",
    "    for i in range(len(B)):\n",
    "        hl[i] = perceptron(X,W=W[2*i:2*i+2], B=B[i:i+1],\n",
    "                            synapticPotential=synapticPotential,\n",
    "                            activation=activation,\n",
    "                            string=string,debug=debug,showHidden=showHidden)\n",
    "    if debug: \n",
    "        print(hl[0],hl[1],hl[2],hl[3])\n",
    "        \n",
    "    return perceptronAND( [perceptronAND( [hl[0],hl[1]] ), perceptronAND( [hl[2],hl[3]] ) ] )\n",
    "\n",
    "\n",
    "## Helper function. called ng_scan2InputNN in previous chapter\n",
    "def scan2InputNN(nn,weights,Biases=None,xysup=11,showHidden=None,debug=False): \n",
    "    \"\"\"\n",
    "    ng_scan2InputNN(nn,weights,xysup=11,showHidden=None,debug=False)\n",
    "    \n",
    "    Same as original scan2InputNN but takes an additional parameter b, the bias of the perceptrons. \n",
    "    The bias b is an array of numbers for each of the different perceptrons the NN is  composed of.\n",
    "    \n",
    "    Pass a 'neural network with 2 inputs and its weights, then scan a given range of values for those inputs and see \n",
    "     where the NN classifies those points into ('-' / '+')\n",
    "     \n",
    "    NN needs to have defined inputs and weights as simple array. Also optional variables showHidden, \n",
    "    for showing values of hidden neurons, or debug for additional info\n",
    "    \n",
    "    xysup=11 #supremum of x,y ranges, with -xysup(1-[1/2]) <= x,y <= xysup(1-[1/2]). \n",
    "    E.g., xysup=11, xy \\in [-5,5]\n",
    "    \n",
    "    It calls the NN as: nn([x1,x2],weights,showHidden=showHidden,debug=debug)\n",
    "    \"\"\"\n",
    "    X1 = np.arange(0,xysup)\n",
    "    X2 = np.arange(0,xysup)\n",
    "    X1 -= floor(xysup/2)\n",
    "    X2 -= floor(xysup/2)\n",
    "    X1\n",
    "\n",
    "    if showHidden: print(\"Showing hidden neuron \",showHidden)\n",
    "        \n",
    "    print(\"      \",end=\"\") #formatting\n",
    "    for x1 in X1:          #Print first line w/ x-coord \n",
    "        print(x1,end=\"  \")\n",
    "    print(\"\\n\\t\",end=\"\") \n",
    "    for x2 in reversed(X2):                               #Print first y coor. (+ values top/first=>reverse array)\n",
    "        print(x2,end=\" \") if x2<0 else print(x2,end=\"  \")\n",
    "        for x1 in X1:                                     #...then go on w/ values along x-axis\n",
    "            #print(nn1HiddenLayer3([x1,x2],W),end=\"  \" )\n",
    "            #if debug: help(nn)\n",
    "            print(nn([x1,x2],weights,Biases,showHidden=showHidden,debug=debug),end=\"  \" )\n",
    "        print(\"\\n\\t\",end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span id=\"secTheTraining\">The Training: First Learning Step</span>[^](#secContent)\n",
    "\n",
    "We would like to define a *training* procedure that would allow any given network to *learn* the values of the weights and biases by simply giving the network a bunch of example inputs and their expected output. Ideally, at the end of the this *training phase* the resulting weights would make the our neural network nail all answers for all inputs.\n",
    "\n",
    "But, **how can the neural network *learn*?** Well, this question can now be recast as **what training steps can we device that, given the examples, yields the right set of weights?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainNN(nn,trainingSet,W,nbInputNeurons=2):\n",
    "    '''\n",
    "    How nn can learn?\n",
    "    X'1, X'2,...X'm (vectors n-dim) input examples + Y'1,...,Y'm (K-dim) outputs\n",
    "    Idea: Let's define a \"learning\"/\"training\" method as follows:\n",
    "    0.- Assume NN topology fixed.\n",
    "    1.- Choose weights at random, small.\n",
    "    2.- Get Yi i=1,..,m output vectors for each input X'i\n",
    "    3.- Decide how good those m outputs are => Criterion ??\n",
    "    4.- If good enough then FINISHED; otherwise: Change new weights (hopefully so as to improve 3)\n",
    "    5.- Go to step 2.\n",
    "\n",
    "    Example of possible Criterion: \\Sum_i (Yi - Y'i)^2 := Euclidean distance\n",
    "\n",
    "    How change weights (step 4)?Newton (Gradient) + Back Propagation\n",
    "\n",
    "    wij(t+1) = wij(t) - \\eta (yi-y'i)*xj\n",
    "    \n",
    "    X1o\n",
    "       \\    \n",
    "        \\w1           __\n",
    "         \\__________ |  _____o y\n",
    "         /          _|\n",
    "        /w2  \n",
    "       /    \n",
    "    X2o\n",
    "      \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Learning the AND function\n",
    "\n",
    "Let's try a quick and dirty procedure. In order to keep things simple, we will set to try learning the AND operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1  0  1  \n",
      "\t1  1  1  1  \n",
      "\t0  1  1  1  \n",
      "\t-1 o  1  1  \n",
      "\tNo info on AND function. NN yields 1 for all four inputs\n",
      "\n",
      "Start learning/training step: weights / bias / Total Error\n",
      "End of iteration i=0 [ 0.09  0.19] [ 0.27]  : E= 3\n",
      "End of iteration i=1 [ 0.08  0.18] [ 0.24]  : E= 3\n",
      "End of iteration i=2 [ 0.07  0.17] [ 0.21]  : E= 3\n",
      "End of iteration i=3 [ 0.06  0.16] [ 0.18]  : E= 3\n",
      "End of iteration i=4 [ 0.05  0.15] [ 0.15]  : E= 3\n",
      "End of iteration i=5 [ 0.04  0.14] [ 0.12]  : E= 3\n",
      "End of iteration i=6 [ 0.03  0.13] [ 0.09]  : E= 3\n",
      "End of iteration i=7 [ 0.02  0.12] [ 0.06]  : E= 3\n",
      "End of iteration i=8 [ 0.01  0.11] [ 0.03]  : E= 3\n",
      "End of iteration i=9 [  1.04083409e-17   1.00000000e-01] [ -1.42247325e-16]  : E= 3\n",
      "End of iteration i=10 [  1.04083409e-17   9.00000000e-02] [-0.01]  : E= 1\n",
      "End of iteration i=11 [  1.04083409e-17   8.00000000e-02] [-0.02]  : E= 1\n",
      "End of iteration i=12 [  1.04083409e-17   7.00000000e-02] [-0.03]  : E= 1\n",
      "End of iteration i=13 [  1.04083409e-17   6.00000000e-02] [-0.04]  : E= 1\n",
      "End of iteration i=14 [ 0.01  0.06] [-0.04]  : E= 2\n",
      "End of iteration i=15 [ 0.01  0.05] [-0.05]  : E= 1\n",
      "End of iteration i=16 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "End of iteration i=17 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "End of iteration i=18 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "End of iteration i=19 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "Finished 20 iterations of learning/training\n",
      "      -1  0  1  \n",
      "\t1  o  o  1  \n",
      "\t0  o  o  o  \n",
      "\t-1 o  o  o  \n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:107: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "## Testing naive training method\n",
    "## Try learning AND function => simple perceptron\n",
    "tset=[([0,0],0),([1,0],0),([0,1],0),([1,1],1)]\n",
    "\n",
    "weights=np.array([0.1,0.2])\n",
    "bias=np.array([0.3])\n",
    "#before learning AND function from examples\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,xysup=3)\n",
    "print('No info on AND function. NN yields 1 for all four inputs',end=\"\\n\\n\")\n",
    "\n",
    "#Start learning\n",
    "print(\"Start learning/training step:\",'weights / bias / Total Error')\n",
    "eta=0.01\n",
    "debug=False\n",
    "nIterations=20\n",
    "for i in range(nIterations): #we repeat. Goal: hopefully after studying example many times it stops making mistakes E=0-\n",
    "    E=0\n",
    "    for x,yt in tset:\n",
    "        if debug: print(x,yt,weights,bias,\" : \",end=\"\")\n",
    "        y=perceptron(X=x,W=weights,B=bias,activation=nonLinearHeaviside,string=False)\n",
    "        E += (y-yt)**2\n",
    "        if debug: print(y,y-yt,E)\n",
    "        xa = np.array(x)\n",
    "        weights -= eta*(y-yt)*xa\n",
    "        bias[0] -= eta*(y-yt)\n",
    "\n",
    "    print(\"End of iteration i=\"+str(i),weights,bias,\" : E=\",E)\n",
    "\n",
    "#After learning\n",
    "print('Finished',nIterations,'iterations of learning/training')\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,xysup=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "    X1o_____________| _o\n",
    "       \\     /\n",
    "        \\   /\n",
    "         \\ /\n",
    "         /\\\n",
    "        /  \\\n",
    "       /    \\\n",
    "      /      \\\n",
    "    X2o-------------| -o\n",
    "\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
