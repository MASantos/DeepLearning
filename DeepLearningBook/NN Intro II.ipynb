{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning. Chapter 2: Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"secContent\">Content</span>\n",
    "\n",
    "1. [Summary of Chap1's code]()\n",
    "2. [The Training: Teaching a perceptron](#secTheTraining)\n",
    "  1. [Example: Learning  the AND function](#secExampleAND)\n",
    "  1. [Example: Learning the OR function](#secExampleOR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Summary of Chap1\n",
    "#\n",
    "#\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "def synapticPotentialWeighted(X,W,b=0):\n",
    "    return np.dot(X,W[:len(X)])+b\n",
    "    \n",
    "def synapticPotentialAverage(X,W=None):\n",
    "    return sum(X)/len(X)\n",
    "    \n",
    "def synapticPotentialMedian(X,W=None):\n",
    "    \"\"\"\n",
    "    To make sense, \n",
    "    WT := \\sum |W_i| := total # of X values (not all distinct) seen.\n",
    "    Assing each X_i to its weight W_i -> (X_i,W_i)\n",
    "    sort by X_i\n",
    "    lx2=floor(WT/2)\n",
    "    if exists j ; |W_j| = lx2 :\n",
    "        md = ( W_j*X_j + W_(j+1) *X_(j+1) ) / ( W_j + W_(j+1)) #weighted average of both points\n",
    "    if WT%2==0: \n",
    "    else:\n",
    "        md\n",
    "        \n",
    "    Example: w1=7 w2=5 w3=2 => WT=14 => lx2=7 => md = (w1*x1+w2*x2 ) / (w1+w2) = (7x1+5x2)/12\n",
    "                  \n",
    "    if lx%2 == 0 ):\n",
    "        md=(Z[lx2-1] + Z[lx2] )*0.5\n",
    "    else:\n",
    "        md=Z[lx2]*1.0 #so that it always returns a float\n",
    "    \"\"\"\n",
    "    \n",
    "    # W=[1,1,...,1] as many 1's as length of X\n",
    "    if W == None: \n",
    "        W=np.ones(len(X)) \n",
    "\n",
    "    #Given array [x1,x2,...xn] -> get the sum_i xi\n",
    "    def foldr(_X,_accu=0):          \n",
    "        if len(_X) == 0: return _accu\n",
    "        if len(_X) == 1: return _accu+_X[0]\n",
    "        return (_accu + _X[-1] + foldr(_X[:-1]) )\n",
    "\n",
    "    WT = foldr( list( map(abs,W) ) )\n",
    "    \n",
    "    def pfilter(par,x):\n",
    "        for a,b in par:\n",
    "            if abs(b) == x: return par.index((a,b))\n",
    "        return None\n",
    "\n",
    "    Z=sorted(list(zip(X,W)))\n",
    "    lx2=floor(WT/2)\n",
    "    #on Z=sorted(X,W): Python will mutate arguments **if it can**\n",
    "               \n",
    "    #if WT%2 == 0:\n",
    "    #    j = pfilter(Z,lx2)\n",
    "    #    try: return (Z[j][1]*Z[j][0]+Z[j+1][1]*Z[j+1][0]) \n",
    "    #    except: print('ERROR j= ',j,\" Z: \",Z, \" WT: \",WT)\n",
    "    #else:\n",
    "    j=0 ; zw=abs(Z[j][1])\n",
    "    while zw < lx2:\n",
    "        j+=1\n",
    "        zw+=abs(Z[j][1]) \n",
    "    if j == len(Z)-1: j-=1\n",
    "    \n",
    "    try: return (Z[j+1][1]*Z[j+1][0]+Z[j][1]*Z[j][0])/(abs(Z[j+1][1])+abs(Z[j][1])) # {wj*xj + w(j+1)*x(j+1)}/(wj+wj+1) \n",
    "    except: print('ERROR j=',j,\" Z: \",Z, \" WT: \",WT)\n",
    "\n",
    "def synapticPotentialMode(X,W):\n",
    "    \"\"\"\n",
    "        To make sense, the weights W should be non-negative.\n",
    "    Choose randomly one among all possible maxima\n",
    "    \n",
    "    When W[1,1] Provides a soft boundary between both phases (classes) with difussion across the boundary\n",
    "    \"\"\"\n",
    "    wmax = max(list(map(abs,W)) )\n",
    "    imax = [ i for i,j in enumerate(list(map(abs,W)) ) if j==wmax ]\n",
    "    #print(\"max weights:\",imax)\n",
    "    i = np.random.choice(imax,1)\n",
    "    return X[i]*W[i]/wmax\n",
    "    \n",
    "##\n",
    "def nonLinearBipolarStep(x,string=None):\n",
    "    if not string: return (-1 if x<0 else 1 )\n",
    "    else: return ('-' if x<0 else '1')\n",
    "\n",
    "def nonLinearHeaviside(x,string=None):\n",
    "    if not string: return (0 if x<0 else 1 )\n",
    "    else: return ('o' if x<0 else '1')\n",
    "\n",
    "def nonLinearRelu(x,string=None):\n",
    "    r = max(0,x)\n",
    "    if not string: return r\n",
    "    else: return str(r)\n",
    "\n",
    "def identity(x,string=None):\n",
    "    if not string: return x\n",
    "    else: return str(x)\n",
    "\n",
    "## General perceptron: called ng_nGenPerceptron in previous Chapter\n",
    "def perceptron(X,W,B=[0],\n",
    "                   synapticPotential=synapticPotentialWeighted,\n",
    "                   #activation=nonLinearBipolarStep,\n",
    "                   activation=nonLinearHeaviside,\n",
    "                   #activation=(lambda z,s: nonLinearBipolarStep(nonLinearRelu(z,None),s)),\n",
    "                   string=True,debug=False,showHidden=False):\n",
    "    if B == None: B=[0]\n",
    "    if debug: print(\"ng_nGenPer: \",synapticPotential.__name__,activation.__name__,\"W:\",W,\" X:\",X,\" B:\",B)    \n",
    "    return activation(synapticPotential(X,W,b=B[0]),string=string)\n",
    "\n",
    "## AND function\n",
    "def perceptronAND(X):\n",
    "    return perceptron(X=X,W=[1,1],B=[2],\n",
    "                             synapticPotential=synapticPotentialWeighted,\n",
    "                             activation=nonLinearHeaviside,\n",
    "                             string=False,debug=False,showHidden=False)\n",
    "\n",
    "## (Feed-forward neural network for XOR function using Bipolar step as activation function\n",
    "##   and weighted sum as aggregator)\n",
    "def nnXOR(X,W,showHidden=None,debug=False):\n",
    "    assert len(W)==6 \n",
    "    strings=[False,False,False,False,False]\n",
    "    h=0\n",
    "    if showHidden : \n",
    "        h=(showHidden-1)%2 # h1-2\n",
    "        assert h>=0\n",
    "        strings[h]=True \n",
    "        \n",
    "    hl=[]\n",
    "    hl.append(perceptron(X=X,W=W[0:2],B=[0],\n",
    "                        synapticPotential=synapticPotentialWeighted,\n",
    "                        activation=nonLinearBipolarStep,\n",
    "                        string=strings[0],debug=debug\n",
    "                        ) \n",
    "             )\n",
    "    hl.append(perceptron(X=X,W=W[2:4],B=[0],\n",
    "                        synapticPotential=synapticPotentialWeighted,\n",
    "                        activation=nonLinearBipolarStep,\n",
    "                        string=strings[1],debug=debug\n",
    "                        ) \n",
    "             )\n",
    "\n",
    "    if showHidden: return hl[h]\n",
    "    \n",
    "    return nPerceptron(X=[hl[0],hl[1]],W=W[4:6],string=True)\n",
    "\n",
    "## Neural network for a convex hull pattern\n",
    "def nnConvexHull(X,W,B=[0,0,0,0],\n",
    "                 synapticPotential=synapticPotentialWeighted,\n",
    "                 activation=nonLinearHeaviside,\n",
    "                 string=None,debug=False,showHidden=False):\n",
    "    \"\"\"\n",
    "    Classifies the 2d plane in two distinct regions with an arbitrary polygonal perimetre\n",
    "    \"\"\"\n",
    "    hl=[0,0,0,0]\n",
    "    for i in range(len(B)):\n",
    "        hl[i] = perceptron(X,W=W[2*i:2*i+2], B=B[i:i+1],\n",
    "                            synapticPotential=synapticPotential,\n",
    "                            activation=activation,\n",
    "                            string=string,debug=debug,showHidden=showHidden)\n",
    "    if debug: \n",
    "        print(hl[0],hl[1],hl[2],hl[3])\n",
    "        \n",
    "    return perceptronAND( [perceptronAND( [hl[0],hl[1]] ), perceptronAND( [hl[2],hl[3]] ) ] )\n",
    "\n",
    "\n",
    "## Helper function. called ng_scan2InputNN in previous chapter. Modified to accept perceptron functions\n",
    "def scan2InputNN(nn,weights,Biases=None,xysup=11,\n",
    "                    synapticPotential=synapticPotentialWeighted,\n",
    "                    activation=nonLinearBipolarStep,\n",
    "                    #activation=nonLinearHeaviside,\n",
    "                    showHidden=None,debug=False): \n",
    "    \"\"\"\n",
    "    ng_scan2InputNN(nn,weights,xysup=11,\n",
    "                    synapticPotential=synapticPotentialWeighted,\n",
    "                    activation=nonLinearBipolarStep,\n",
    "                    showHidden=None,debug=False)\n",
    "    \n",
    "    Same as original scan2InputNN but takes an additional parameter b, the bias of the perceptrons. \n",
    "    The bias b is an array of numbers for each of the different perceptrons the NN is  composed of.\n",
    "    \n",
    "    Pass a 'neural network with 2 inputs and its weights, then scan a given range of values for those inputs and see \n",
    "     where the NN classifies those points into ('-' / '+')\n",
    "     \n",
    "    NN needs to have defined inputs and weights as simple array. Also optional variables showHidden, \n",
    "    for showing values of hidden neurons, or debug for additional info\n",
    "    \n",
    "    xysup=11 #supremum of x,y ranges, with -xysup(1-[1/2]) <= x,y <= xysup(1-[1/2]). \n",
    "    E.g., xysup=11, xy \\in [-5,5]\n",
    "    \n",
    "    It calls the NN as: nn([x1,x2],weights,showHidden=showHidden,debug=debug)\n",
    "    \"\"\"\n",
    "    X1 = np.arange(0,xysup)\n",
    "    X2 = np.arange(0,xysup)\n",
    "    X1 -= floor(xysup/2)\n",
    "    X2 -= floor(xysup/2)\n",
    "    X1\n",
    "\n",
    "    if showHidden: print(\"Showing hidden neuron \",showHidden)\n",
    "        \n",
    "    print(\"      \",end=\"\") #formatting\n",
    "    for x1 in X1:          #Print first line w/ x-coord \n",
    "        print(x1,end=\"  \")\n",
    "    print(\"\\n\\t\",end=\"\") \n",
    "    for x2 in reversed(X2):                               #Print first y coor. (+ values top/first=>reverse array)\n",
    "        print(x2,end=\" \") if x2<0 else print(x2,end=\"  \")\n",
    "        for x1 in X1:                                     #...then go on w/ values along x-axis\n",
    "            #print(nn1HiddenLayer3([x1,x2],W),end=\"  \" )\n",
    "            #if debug: help(nn)\n",
    "            print(nn([x1,x2],weights,Biases,\n",
    "                     synapticPotential=synapticPotential,\n",
    "                     activation=activation,\n",
    "                     showHidden=showHidden,debug=debug),end=\"  \" )\n",
    "        print(\"\\n\\t\",end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span id=\"secTheTraining\">The Training: First Learning Step</span>[^](#secContent)\n",
    "\n",
    "We would like to define a *training* procedure that would allow any given network to *learn* the values of the weights and biases by simply giving the network a bunch of example inputs and their expected output. Ideally, at the end of this *training phase* the resulting weights would make our neural network nail all answers for all inputs.\n",
    "\n",
    "But, **how can the neural network *learn*?** Well, this question can now be recast as **what training steps can we device that, given the examples, yield the right set of weights?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainNN(nn,trainingSet,W,nbInputNeurons=2):\n",
    "    '''\n",
    "    How nn can learn?\n",
    "    X'1, X'2,...X'm (vectors n-dim) input examples + Y'1,...,Y'm (K-dim) outputs\n",
    "    Idea: Let's define a \"learning\"/\"training\" method as follows:\n",
    "    0.- Assume NN topology fixed.\n",
    "    1.- Choose weights at random, small.\n",
    "    2.- Get Yi i=1,..,m output vectors for each input X'i\n",
    "    3.- Decide how good those m outputs are => Criterion ??\n",
    "    4.- If good enough then FINISHED; otherwise: Change new weights (hopefully so as to improve 3)\n",
    "    5.- Go to step 2.\n",
    "\n",
    "    Example of possible Criterion: \\Sum_i (Yi - Y'i)^2 := Euclidean distance\n",
    "\n",
    "    How change weights (step 4)?Newton (Gradient) + Back Propagation\n",
    "\n",
    "    wij(t+1) = wij(t) - \\eta (yi-y'i)*xj\n",
    "    \n",
    "    X1o\n",
    "       \\    \n",
    "        \\w1           __\n",
    "         \\__________ |  _____o y\n",
    "         /          _|\n",
    "        /w2  \n",
    "       /    \n",
    "    X2o\n",
    "      \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"secExampleAND\">Example: Learning the AND function</span>[^](#secContent)\n",
    "\n",
    "Let's try a quick and dirty procedure first. In order to keep things simple, we will set to try learning the AND operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1  0  1  \n",
      "\t1  1  1  1  \n",
      "\t0  1  1  1  \n",
      "\t-1 o  1  1  \n",
      "\tNo info on AND function. NN yields 1 for all four inputs\n",
      "\n",
      "Start learning/training step: weights / bias / Total Error\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=0 [ 0.09  0.19] [ 0.27]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=1 [ 0.08  0.18] [ 0.24]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=2 [ 0.07  0.17] [ 0.21]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=3 [ 0.06  0.16] [ 0.18]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=4 [ 0.05  0.15] [ 0.15]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=5 [ 0.04  0.14] [ 0.12]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=6 [ 0.03  0.13] [ 0.09]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=7 [ 0.02  0.12] [ 0.06]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=8 [ 0.01  0.11] [ 0.03]  : E= 3\n",
      "[0, 0] 1 1 1\n",
      "[1, 0] 1 1 2\n",
      "[0, 1] 1 1 3\n",
      "[1, 1] 1 0 3\n",
      "End of iteration i=9 [  1.04083409e-17   1.00000000e-01] [ -1.42247325e-16]  : E= 3\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 1 0 1\n",
      "End of iteration i=10 [  1.04083409e-17   9.00000000e-02] [-0.01]  : E= 1\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 1 0 1\n",
      "End of iteration i=11 [  1.04083409e-17   8.00000000e-02] [-0.02]  : E= 1\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 1 0 1\n",
      "End of iteration i=12 [  1.04083409e-17   7.00000000e-02] [-0.03]  : E= 1\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 1 0 1\n",
      "End of iteration i=13 [  1.04083409e-17   6.00000000e-02] [-0.04]  : E= 1\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 0 -1 2\n",
      "End of iteration i=14 [ 0.01  0.06] [-0.04]  : E= 2\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 1 1 1\n",
      "[1, 1] 1 0 1\n",
      "End of iteration i=15 [ 0.01  0.05] [-0.05]  : E= 1\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 0 0 0\n",
      "[1, 1] 1 0 0\n",
      "End of iteration i=16 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 0 0 0\n",
      "[1, 1] 1 0 0\n",
      "End of iteration i=17 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 0 0 0\n",
      "[1, 1] 1 0 0\n",
      "End of iteration i=18 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "[0, 0] 0 0 0\n",
      "[1, 0] 0 0 0\n",
      "[0, 1] 0 0 0\n",
      "[1, 1] 1 0 0\n",
      "End of iteration i=19 [ 0.01  0.05] [-0.05]  : E= 0\n",
      "Finished 20 iterations of learning/training   eta:  0.01  : weights: [ 0.01  0.05]  : bias: -0.05\n",
      "      -1  0  1  \n",
      "\t1  o  o  1  \n",
      "\t0  o  o  o  \n",
      "\t-1 o  o  o  \n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:107: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "## Testing naive training method\n",
    "## Try learning AND function => simple perceptron\n",
    "\n",
    "tset=[([0,0],0),([1,0],0),([0,1],0),([1,1],1)]\n",
    "\n",
    "weights=np.array([0.1,0.2])\n",
    "bias=np.array([0.3])\n",
    "#before learning AND function from examples\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,activation=nonLinearHeaviside,xysup=3)\n",
    "print('No info on AND function. NN yields 1 for all four inputs',end=\"\\n\\n\")\n",
    "\n",
    "#Start learning\n",
    "print(\"Start learning/training step:\",'weights / bias / Total Error')\n",
    "eta=0.01\n",
    "debug=1\n",
    "nIterations=20\n",
    "for i in range(nIterations): #we repeat. Goal: hopefully after studying example many times it stops making mistakes E=0-\n",
    "    E=0\n",
    "    for x,yt in tset:\n",
    "        if debug>1: print(x,yt,weights,bias,\" : \",end=\"\")\n",
    "        y=perceptron(X=x,W=weights,B=bias,activation=nonLinearHeaviside,string=False,debug=0)\n",
    "        E += (y-yt)**2\n",
    "        if debug: print(x,y,(y-yt),E)\n",
    "        xa = np.array(x)\n",
    "        weights -= eta*(y-yt)*xa\n",
    "        bias[0] -= eta*(y-yt)\n",
    "\n",
    "    print(\"End of iteration i=\"+str(i),weights,bias,\" : E=\",E)\n",
    "\n",
    "#After learning\n",
    "print('Finished',nIterations,'iterations of learning/training','  eta: ',eta,' : weights:',weights,' : bias:',bias[0])\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,activation=nonLinearHeaviside,xysup=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations are in order. We managed to have our *single neuron learn* the AND logical operator just by giving it a few (only four) examples!\n",
    "\n",
    "Let's discuss what the neuron learned. We know the percpetron learns to classify points in the plane by simply drawing a straight line. What line did our perceptron learned? The final weights are $[w_1=0.01,w_2=0.05]$ and the bias learned is $b=-0.05$. The line is determined by the points $(x,y)$ such that $x_1\\cdot w_1\\,+\\,x_2\\cdot w_2\\,+\\,b\\,=\\,0$. Clearly, this equation for the line is the same if we multiply all weights **and** the bias by the same factor $\\alpha$. Let's say we multiply by $\\alpha=100$. The line obtained is \n",
    "$$x_1\\,+\\,5\\cdot x_2\\,-\\,5\\,=\\,0$$\n",
    "Thus, for $x_1=0\\,\\rightarrow\\,x_2\\,=1$ and for  $x_1=1\\,\\rightarrow\\,x_2\\,=\\,4/5\\,=\\,0.8$. That is, it's the line that passes through points $(0,1)$ and $(1,0.8)$.\n",
    "\n",
    "But, wait a second...This line **does not** reproduce the AND function! This is because for the point $(0,1)$ the aggregator function with these weights will yield a value of zero to the step function, which in turn will give a $1$. But the AND function is 0 for this point!\n",
    "\n",
    "Did the learning go wrong? No it didn't. The problem here lies in how Python rounds off numbers for displaying them. We can have a closer look (further decimals) at the weights obtained with the function ```Decimal``` from the module ```decimal``` \n",
    "```\n",
    "from decimal import Decimal\n",
    "list(map(Decimal,weights)),list(map(Decimal,bias))\n",
    "[Decimal('0.01000000000000001061650767297805941780097782611846923828125'),\n",
    "  Decimal('0.04999999999999997501998194593397784046828746795654296875')],\n",
    " [Decimal('-0.050000000000000148492329543614687281660735607147216796875')]\n",
    "```\n",
    "If we don't *look* at the weights values, but just pass them along, all is good.\n",
    "\n",
    "Whence, the **actual line learned** is one the passes just **slightly above the point $(0,1$** and somewhere between points $(1,0)$ and $(1,1)$. As the weights are positive this is indeed a valid separation line. **Our perceptron did indeed learn things right**!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:107: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('o', 'o', 'o', '1', array([ 0.01,  0.05]), array([-0.05]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myAND(X):\n",
    "    return perceptron(X=X,W=weights,B=bias,activation=nonLinearHeaviside)\n",
    "\n",
    "myAND([0.,0.]), myAND([0.,1.0]), myAND([1.0,0.]), myAND([1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Decimal('0.01000000000000001061650767297805941780097782611846923828125'),\n",
       "  Decimal('0.04999999999999997501998194593397784046828746795654296875')],\n",
       " [Decimal('-0.050000000000000148492329543614687281660735607147216796875')])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "list(map(Decimal,weights)),list(map(Decimal,bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"secExampleOR\">Example: Learning the OR function</span>[^](#secContent)\n",
    "What else can we *learn* with only one neuron?  What about the OR logical operator? This is kind of the complementary of the AND one as it is 0 (False) only when both entries are 0, otherwise it output 1 (True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1  0  1  \n",
      "\t1  1  1  1  \n",
      "\t0  1  1  1  \n",
      "\t-1 o  1  1  \n",
      "\tNo info on OR function. NN yields 1 for all four inputs\n",
      "\n",
      "Start learning/training step: weights / bias / Total Error\n",
      "End of iteration i=0 [ 0.1  0.2] [ 0.29]  : E= 1\n",
      "End of iteration i=1 [ 0.1  0.2] [ 0.28]  : E= 1\n",
      "End of iteration i=2 [ 0.1  0.2] [ 0.27]  : E= 1\n",
      "End of iteration i=3 [ 0.1  0.2] [ 0.26]  : E= 1\n",
      "End of iteration i=4 [ 0.1  0.2] [ 0.25]  : E= 1\n",
      "End of iteration i=5 [ 0.1  0.2] [ 0.24]  : E= 1\n",
      "End of iteration i=6 [ 0.1  0.2] [ 0.23]  : E= 1\n",
      "End of iteration i=7 [ 0.1  0.2] [ 0.22]  : E= 1\n",
      "End of iteration i=8 [ 0.1  0.2] [ 0.21]  : E= 1\n",
      "End of iteration i=9 [ 0.1  0.2] [ 0.2]  : E= 1\n",
      "End of iteration i=10 [ 0.1  0.2] [ 0.19]  : E= 1\n",
      "End of iteration i=11 [ 0.1  0.2] [ 0.18]  : E= 1\n",
      "End of iteration i=12 [ 0.1  0.2] [ 0.17]  : E= 1\n",
      "End of iteration i=13 [ 0.1  0.2] [ 0.16]  : E= 1\n",
      "End of iteration i=14 [ 0.1  0.2] [ 0.15]  : E= 1\n",
      "End of iteration i=15 [ 0.1  0.2] [ 0.14]  : E= 1\n",
      "End of iteration i=16 [ 0.1  0.2] [ 0.13]  : E= 1\n",
      "End of iteration i=17 [ 0.1  0.2] [ 0.12]  : E= 1\n",
      "End of iteration i=18 [ 0.1  0.2] [ 0.11]  : E= 1\n",
      "End of iteration i=19 [ 0.1  0.2] [ 0.1]  : E= 1\n",
      "End of iteration i=20 [ 0.1  0.2] [ 0.09]  : E= 1\n",
      "End of iteration i=21 [ 0.1  0.2] [ 0.08]  : E= 1\n",
      "End of iteration i=22 [ 0.1  0.2] [ 0.07]  : E= 1\n",
      "End of iteration i=23 [ 0.1  0.2] [ 0.06]  : E= 1\n",
      "End of iteration i=24 [ 0.1  0.2] [ 0.05]  : E= 1\n",
      "End of iteration i=25 [ 0.1  0.2] [ 0.04]  : E= 1\n",
      "End of iteration i=26 [ 0.1  0.2] [ 0.03]  : E= 1\n",
      "End of iteration i=27 [ 0.1  0.2] [ 0.02]  : E= 1\n",
      "End of iteration i=28 [ 0.1  0.2] [ 0.01]  : E= 1\n",
      "End of iteration i=29 [ 0.1  0.2] [ -1.42247325e-16]  : E= 1\n",
      "End of iteration i=30 [ 0.1  0.2] [ -1.42247325e-16]  : E= 0\n",
      "End of iteration i=31 [ 0.1  0.2] [ -1.42247325e-16]  : E= 0\n",
      "End of iteration i=32 [ 0.1  0.2] [ -1.42247325e-16]  : E= 0\n",
      "Finished 33 iterations of learning/training   eta:  0.01  : weights: [ 0.1  0.2]  : bias: -1.4224732503e-16\n",
      "      -1  0  1  \n",
      "\t1  1  1  1  \n",
      "\t0  o  o  1  \n",
      "\t-1 o  o  o  \n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:107: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "## Testing naive training method\n",
    "## Try learning OR function => simple perceptron\n",
    "tset=[([0,0],0),([1,0],1),([0,1],1),([1,1],1)]\n",
    "\n",
    "weights=np.array([0.1,0.2])\n",
    "bias=np.array([0.3])\n",
    "#before learning OR function from examples\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,activation=nonLinearHeaviside,xysup=3)\n",
    "print('No info on OR function. NN yields 1 for all four inputs',end=\"\\n\\n\")\n",
    "\n",
    "#Start learning\n",
    "print(\"Start learning/training step:\",'weights / bias / Total Error')\n",
    "eta=0.01\n",
    "debug=0\n",
    "nIterations=33\n",
    "for i in range(nIterations): #we repeat. Goal: hopefully after studying example many times it stops making mistakes E=0-\n",
    "    E=0\n",
    "    for x,yt in tset:\n",
    "        if debug>1: print(x,yt,weights,bias,\" : \",end=\"\")\n",
    "        y=perceptron(X=x,W=weights,B=bias,activation=nonLinearHeaviside,string=False,debug=0)\n",
    "        E += (y-yt)**2\n",
    "        if debug: print(x,y,(y-yt),E)\n",
    "        xa = np.array(x)\n",
    "        weights -= eta*(y-yt)*xa\n",
    "        bias[0] -= eta*(y-yt)\n",
    "\n",
    "    print(\"End of iteration i=\"+str(i),weights,bias,\" : E=\",E)\n",
    "\n",
    "#After learning\n",
    "print('Finished',nIterations,'iterations of learning/training','  eta: ',eta,' : weights:',weights,' : bias:',bias[0])\n",
    "scan2InputNN(perceptron,weights=weights,Biases=bias,activation=nonLinearHeaviside,xysup=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this time we needed more iterations to perfectly learn the OR function with one single neuron.\n",
    "\n",
    "In this case, the numerical precision used for displaying is not giving us problems, or not at first sight. For the casual, but educated reader, a value of $-1.42247325\\cdot 10^{-16}$ is tantamount to $0$. However, if we take it zero **then the learning is wrong** as the line would contain the point $(0,0)$ and thus classify it as $1$ instead of $0$ -as the OR function does. \n",
    "\n",
    "Indeed, the bias is actually **smaller than zero** (although by a minuscule amount) and the weights are $w_1=1,\\,w_2=2$, where we used our freedom to multiply all weights and bias by a common factor. Being the bias negative means the line passes above the point $(0,0)$; as it is very small, it means the line passes very close to that point -but without touching it! Whence all other three points lie above the line and are classified as $1$ -exactly as they should!\n",
    "\n",
    "Again, thus, our single neuron managed to learn something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Decimal('0.1000000000000000055511151231257827021181583404541015625'),\n",
       "  Decimal('0.200000000000000011102230246251565404236316680908203125')],\n",
       " [Decimal('-1.422473250300981817417778074741363525390625E-16')])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(Decimal,weights)),list(map(Decimal,bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "    X1o_____________| _o\n",
    "       \\     /\n",
    "        \\   /\n",
    "         \\ /\n",
    "         /\\\n",
    "        /  \\\n",
    "       /    \\\n",
    "      /      \\\n",
    "    X2o-------------| -o\n",
    "\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
