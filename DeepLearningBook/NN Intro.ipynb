{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning, aka., Neural Networks\n",
    "\n",
    "This notebook is intended as a lean, hands-on introduction to neural networks with the goal of acquiring an intuition on the reason of the fundamental components that are put together in order to any of the deep learning networks that achieve such impressive results as autonomously driving cars, beating any Go master or learning to paint as an all-time master, as the following video showcases.\n",
    "\n",
    "I've also used partially in my Computer Science class at Dragon Academy (Toronto) as a background context in teaching programming, e.g., for motivating the use of things like different data structures, modularity, or higher-order functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For best viewing experience, download this document and open it directly in your own jupyter notebook. Otherwise, if you get a weird line wrapping when reading this python notebook directly from its github page, you may try to zoom out in your browser for this page (```ctrl++```). That may specially be helpful for the diagrams drawn below. Another option is to view this notebook directly on the jupyter site\n",
    "http://nbviewer.jupyter.org/github/MASantos/DeepLearning/blob/DeepLearningBook/DeepLearningBook/NN%20Intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Two minute papers: DNN learns Van gogh's art](TwoMinutesPaperDNNVanGoh.png)](https://www.youtube.com/watch?v=-R9bJGNHltQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gist of Learning\n",
    "\n",
    "The most essential step in learning is distinguishing two distinct groups among a given collection of objects. This is at its root a non-linear process. The very same classification we obtain entails a non-linearity: \"*on one side everything is smooth and green; on the other side everything is smooth but red*\". We will take it as a principle that *it is this process that is the gist of human learning and reasoning capabilities*[<sup id=\"ref0\">+</sup>](#f0).\n",
    "\n",
    "The simplest mathematical and computational toy model that is capable of making such distinctions is the **perceptron**. This is the most simple step which **all neural networks** build upon. It is in fact thus the \"hello world\" example of a neural network.\n",
    "\n",
    "[<sup id=\"f0\">+</sup>] The rationale for such a principle is based on the evolution of Physics and Mathematics during the XX and early XXI centuries. The classification of chemical elements, as explained by Quantum Physics, is intimately related to the atoms being invariant under certain type of transformations. This is an instance of what's called a symmetry of the system. Indeed in modern Physics, as given by Quantum Field Theory and the Standard Model, it is the symmetries of the elementary particles that determine their interactions! The classification of some elementary particles (some baryons and mesons) into families by physicist Murray Gell-Mann is an example where the concept of symmetry is used in the clustering of different objects into distinct groups. The so-called $SU(3)$ symmetry was at the heart of such classification. Furhtermore, Felix Klein, at the beginning of the XX century showed how Geometry can be derived from the symmtry transformations we consider (Euclidean geometry is determined by Affine transformations -rotations, reflections, invertions and translations-, Projective Geometry is determined by Homographies -e.g. fractional transformations). In this context, both the features that make sense to distinguish as well as the objects themselves are given by those properties (e.g., distance, angle, projection,...) and arrangments (read figures) that are invariant under certain transformations (symmetry). It is this invariance that classifies a triangle as something distinct from a square, a pentagon, an hexagon...Finally, modern foundations of Mathematics rely on the concept of *morphism*. This could be viewed as a stripped down version of a symmetry transformation. It allows the grouping of different collections mathematical objects into categories where those objects who belong to any given one are related by a morphism (think of an arrow or an edge) of a given type. For instance, the collection of all sets is *grouped* into the category called **Set** where the morphisms are simply bijective functions. Whence, the common pattern in al these cases is the intimate relation between symmetry/morphism and classifying into different groups. Finally, the Langland's Program in Mathematics, a quest for the unifying  \"Theory of Everything\" in mathematis is in a way a quest for the ultimate symmetries allowing such unified view of all *distinct domains* of Mathematics (Geometry, Number Theory, Harmonic Analysis, Representation theory, etc. Given that ultimately all sciences are a reflection of how human brain and intelligence work. it doesn't seem arbitrary to raise the claim that clustering is at the heart of human understanding and intelligence in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "<pre>\n",
    "2 inputs                                     N inputs\n",
    "\n",
    "X1o                                          X1o\n",
    "   \\                                             \\\n",
    "    \\W1                                           \\\n",
    "     \\             --                              \\             --\n",
    "      ----------  |   --------o y            X2o--------------  |   --------o y\n",
    "     /          __|                          .     /          __|\n",
    "    /W2                                      .    /            \n",
    "X2o/                                         .   /\n",
    "                                                /\n",
    "         |x2                                Xno/ \n",
    "     \\ + |                                    \n",
    "      \\  |  (w1, w2)\n",
    "     - \\ | /^  +\n",
    "   _____\\|/_________ x1\n",
    "         |\\\n",
    "     -   | \\   +\n",
    "         |  \\\n",
    "\n",
    "The simple perceptron is capable of classifying points (x1, x2) (or of making distinctions) by drawing\n",
    "a line. This, in effect, generates a non-linearity in our view of the universe: things on \"the left\" \n",
    "side of the line and things on \"the right\" side. The line has a perpendicular given by the vector \n",
    "(w1, w2). The latter points in the direction where the output y is larger in the algebraic sense \n",
    "(smaller if the non-linear function is an inversion). In the case of N inputs, the perceptron \n",
    "classifies points in an N-dimensional space by drawing an (N-1)-hyperplane. For N>=3, it classifies \n",
    "them between those \"inside\"  that hyperplane and those outside it.\n",
    "</pre> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearity\n",
    "\n",
    "In the above plot, the symbol denoted by\n",
    "```\n",
    "          __\n",
    "      ___|____\n",
    "       __|\n",
    "  \n",
    "```\n",
    "represents a non-linear function. In layman terms, this is a function that is not a straight line. In other terms, at some point it shows a \"jump\" *relative to its value at another point* even \n",
    "though elsewhere it may run as smooth as it possibly can be.\n",
    "\n",
    "You may raise the following concern: \"*If you said that learning entails a non-linear process and you build into \n",
    "the model a non-linear function, it would seem you are cheating, as you'd be 'inserting' into the machine the \n",
    "very essencial process we expect it to perform*\".\n",
    "\n",
    "Indeed, if we would cut off the non-linear function the perceptron would loose its **\"learning\" capability of\n",
    "_classifying_**. However, all is not lost!\n",
    "\n",
    "In such case, the toy model reduces to\n",
    "<pre>\n",
    "X1o                                          \n",
    "   \\                                          \n",
    "    \\W1                                          \n",
    "     \\                                         \n",
    "      --------------o y            \n",
    "     /                                    \n",
    "    /W2                                                 \n",
    "X2o/                                         \n",
    "</pre>\n",
    "Mathematically, this model corresponds to a weighted average of both inputs. Is this _learning_? Well, it is \n",
    "as much as it is learning what we do when we obtain the average percentage of price increase over the year \n",
    "**in order to get an insight into the amount of inflation during said period**. In statistical parlance, this\n",
    "corresponds to calculating the well-known estimator of central tendency called the *mean* or *average* of a \n",
    "sample. Indeed, **this number abstracts a general trend out of all the details given by all measurements in \n",
    "the sample**. \n",
    "\n",
    "While such a process is usually considered an example of learning[<sup id=\"ref1\">1</sup>](#f1), it is a very limiting one. We will see that \n",
    "not being able to classify renders this model incapable of computing any general function: it can calculate \n",
    "one and only one function, namely that weighted average.\n",
    "\n",
    "You could still reply: \"*OK, but putting aside such a limiting case of learning, I think basically you are \n",
    "cheating when you include that non-linear function into your model for the reason mentioned above. Furthermore,\n",
    "I've read about current cutting-edge research on computational systems built using chemical reactions or using \n",
    "DNA molecules or based on complex systems showing self-organization, etc. that [apparently] would not be using \n",
    "an ad-hoc inserted non-linearity as you do. The learning, and thus the knowledge, is an **emergent thing** that comes out of the system in these cases*\".\n",
    "\n",
    "Fair enough! I'm building that non-linearity ad-hoc into my model. Mea culpa. \n",
    "\n",
    "In my defense, though, the details of how a sytem in nature may have developed and implemented such a \n",
    "non-linear response is not that relevant right now. We still have the task of understanding how such general \n",
    "learning and computational capabilities can be modelled using a perceptron. In other terms, there is a long way to go and we must take it one step at a time. \n",
    "\n",
    "The very first such steps is indeed understanding how we can model any computable function using such a simple \n",
    "toy model of a perceptron as a building block. In layman terms, this would give us the analog of a computer.\n",
    "\n",
    "In other words, given any specific problem the first thing we need to learn is **how to devise a neural \n",
    "network that solves my problem**. This is the gist of almost all literature on neural networks (or deep \n",
    "learning, using the new buzz word). \n",
    "\n",
    "One way of stating this problem is then: Given a set of input values and their possible outputs, how can I \n",
    "device the internals of a deep learning network that (1) learns that mapping out of the examples provided,  \n",
    "(2) correctly applies the function learned on a new set of input values not used before during the learning \n",
    "phase, and (3) that make sensible \"generalizations\",i.e. gives reasonable values for cases further away from \n",
    "those used during learning and testing.\n",
    "\n",
    "It is in this context where the details of the non-linear function I put in there become basically irrelevant. Any \n",
    "non-linear function will do the job. In everyday life, the details, however, may be important, but only\n",
    "because our society puts so much emphasis in money and time: different choices of non-linear functions may make the learning process much slower/require many more examples for achieving the same accuracy, or be less acurrate given a fixed amount of available examples. That is, from a conceptual point of view, it doesn't\n",
    "matter which non-linear function we use, but in practical cases, time and/or money will impose some\n",
    "constraints.\n",
    "\n",
    "<sup id=\"f1\">[1]</sup>It's the limiting case of a [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis) for when the number of independent variables is zero and the functional regression relation reduces to a constant.[^](#ref1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear functions\n",
    "\n",
    "These are also called *activation functions*. Here just a few examples.\n",
    "\n",
    "#### Step functions\n",
    "\n",
    "For instance, the **bipolar step-function**: -1 for any strictly negative value of its arguments and +1 for the rest, including zero.\n",
    "<pre>\n",
    "           y\n",
    "         +1|______   \n",
    "           | \n",
    "     ______|______ x  \n",
    "           |\n",
    "      _____|-1\n",
    "           |\n",
    "           \n",
    "</pre>\n",
    "\n",
    "\n",
    "\n",
    "Another is the so-called **Heaviside step-function**: For any negative value of its arguments it yields 0; for all others, +1.\n",
    "<pre>\n",
    "           y\n",
    "         +1|______   \n",
    "           | \n",
    "   0 ______|______ x  \n",
    "           |\n",
    "           |\n",
    "           \n",
    "</pre>\n",
    "\n",
    "\n",
    "\n",
    "####  The rectifier linear unit\n",
    "\n",
    "This is a function that lets all non-negative values pass *as they are* (identity function). However, it zeros all negative values. It looks like this\n",
    "<pre>\n",
    "           y\n",
    "           |  / y=x  \n",
    "     y=0   | / \n",
    "      _____|/_____ x  \n",
    "           |\n",
    "           \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonLinearBipolarStep(x,string=None):\n",
    "    if not string: return (-1 if x<0 else 1 )\n",
    "    else: return ('-' if x<0 else '1')\n",
    "\n",
    "def nonLinearHeaviside(x,string=None):\n",
    "    if not string: return (0 if x<0 else 1 )\n",
    "    else: return ('o' if x<0 else '1')\n",
    "\n",
    "def nonLinearRelu(x,string=None):\n",
    "    r = max(0,x)\n",
    "    if not string: return r\n",
    "    else: return str(r)\n",
    "\n",
    "def identity(x,string=None):\n",
    "    if not string: return x\n",
    "    else: return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perceptron(x1,x2,w1,w2,string=False,debug=False, activation=nonLinearBipolarStep):\n",
    "    \"\"\"\n",
    "    Simple perceptron w/ 2 inputs, 2 weights and Heaviside step function\n",
    "    \"\"\"\n",
    "    z = x1*w1+x2*w2\n",
    "    if debug: print(\"x1: \",x1,\"  x2: \",x2,\"  w1: \",w1,\"  w2: \",w2,\"  z: \",z,end=\" -->  \")\n",
    "    return activation(z,string)\n",
    "    \"\"\"\n",
    "    if not str: return -1 if z<0 else 1;\n",
    "    else: return \"-\" if z<0 else \"1\";\n",
    "    \"\"\"\n",
    "print(perceptron(-0,2,0,1))\n",
    "\n",
    "def nPerceptron(X,W,string=False,debug=False,activation=nonLinearBipolarStep):\n",
    "    assert(len(X)==len(W))\n",
    "    z=0;\n",
    "    for i in range(0, len(X)):\n",
    "        z+=X[i]*W[i]\n",
    "    return activation(z,string)\n",
    "\n",
    "nPerceptron([3,2,2],[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the plane\n",
    "\n",
    "A single perceptron is able to draw a distinction between two parts of the plane. We will use this to build a classifications of the plane beyond that given by a simple one. These are non-linear classifications of the plane, so called right because the boundaries of the domains defined such way a not straight lines.\n",
    "\n",
    "In order to do so we will use the simple perceptron as a kind of a Lego piece. By plugging multiple perpceptrons together we hope to be able to make more sophisticated distinctions, aka, classifications. Such a setup of multiple perceptrons has been known since the middle of the XX century by differents like Multilayer Perceptron, Neural Network or, in the new parlance, Deep Learning.\n",
    "\n",
    "Let's define a function ```scan2InputNN``` that takes any kind of neural network with only two inputs and outputs a map of the plane with the network output at each point. This will give us a simple graphical view of how that neural network classifies the plane. \n",
    "\n",
    "Here the set of weights is fixed beforehand. The goal is simply to get some intuition on how the neuron divides the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "def scan2InputNN(nn,weights,xysup=11,showHidden=None,debug=False): \n",
    "    \"\"\"\n",
    "    scan2InputNN(nn,weights,xysup=11,showHidden=None,debug=False)\n",
    "    \n",
    "    Pass a 'neural network with 2 inputs and its weights, then scan a given range of values for those inputs and see \n",
    "     where the NN classifies those points into ('-' / '+')\n",
    "     \n",
    "    NN needs to have defined inputs and weights as simple array. Also optional variables showHidden, \n",
    "    for showing values of hidden neurons, or debug for additional info\n",
    "    \n",
    "    xysup=11 #supremum of x,y ranges, with -xysup(1-[1/2]) <= x,y <= xysup(1-[1/2]). \n",
    "    E.g., xysup=11, xy \\in [-5,5]\n",
    "    \n",
    "    It calls the NN as: nn([x1,x2],weights,showHidden=showHidden,debug=debug)\n",
    "    \"\"\"\n",
    "    X1 = np.arange(0,xysup)\n",
    "    X2 = np.arange(0,xysup)\n",
    "    X1 -= floor(xysup/2)\n",
    "    X2 -= floor(xysup/2)\n",
    "    X1\n",
    "\n",
    "    if showHidden: print(\"Showing hidden neuron \",showHidden)\n",
    "        \n",
    "    print(\"      \",end=\"\") #formatting\n",
    "    for x1 in X1:          #Print first line w/ x-coord \n",
    "        print(x1,end=\"  \")\n",
    "    print(\"\\n\\t\",end=\"\") \n",
    "    for x2 in reversed(X2):                               #Print first y coor. (+ values top/first=>reverse array)\n",
    "        print(x2,end=\" \") if x2<0 else print(x2,end=\"  \")\n",
    "        for x1 in X1:                                     #...then go on w/ values along x-axis\n",
    "            #print(nn1HiddenLayer3([x1,x2],W),end=\"  \" )\n",
    "            #if debug: help(nn)\n",
    "            print(nn([x1,x2],weights,showHidden=showHidden,debug=debug),end=\"  \" )\n",
    "        print(\"\\n\\t\",end=\"\")\n",
    "\n",
    "\"\"\"def scan2InputPerceptron(ptron,weights,debug=False):\n",
    "    def wraptron(X,weights,showHidden=None):\n",
    "        ptron(X[0],X[1],weights[0],weights[1],string=True,debug=debug)\n",
    "    \n",
    "    scan2InputNN(wraptron,weights)\n",
    "\"\"\"    \n",
    "def wraptron(X,weights,showHidden=None,debug=False):\n",
    "    \"\"\"\n",
    "    A wrapper around the perceptron as defined above in order to be used with scan2InputtNN\n",
    "    \"\"\"\n",
    "    return perceptron(X[0],X[1],weights[0],weights[1],string=True,debug=debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  -  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  -  -  1  1  1  1  1  1  1  1  1  \n",
      "\t1  -  -  -  -  1  1  1  1  1  1  1  \n",
      "\t0  -  -  -  -  -  1  1  1  1  1  1  \n",
      "\t-1 -  -  -  -  -  -  -  1  1  1  1  \n",
      "\t-2 -  -  -  -  -  -  -  -  1  1  1  \n",
      "\t-3 -  -  -  -  -  -  -  -  -  -  1  \n",
      "\t-4 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-5 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\tx1:  -4   x2:  -3   w1:  1   w2:  1   z:  -7 -->  -1\n"
     ]
    }
   ],
   "source": [
    "#scan2InputPerceptron(perceptron,weights=[0,1],debug=True)\n",
    "scan2InputNN(wraptron,weights=[2,3],debug=False)\n",
    "print(perceptron(-4,-3,1,1,debug=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turing complete: XOR gate\n",
    "\n",
    "During the XX century the idea that reasoning could be grasped by an automatic computing machine was quite \n",
    "widespread. The SciFi literature and movies constantly pictures even in our days computing machines as \n",
    "having become a sentient being, or at least an independent reasoning being. These are the androids. \n",
    "\n",
    "It's still unclear what conciousness is, even less so whether we'll be able to build a machine that gains \n",
    "such. However, a key feature we would like to simulate is computational capabilities. In particular, the \n",
    "posibility of solving any problem that is computationally solvable. We need a Turing-complete machine. \n",
    "\n",
    "A basic neural network based on the perceptron can be shown to bear the essence of computation. Indeed, all\n",
    "propositional calculus can be build on one single logica operator (gate), e.g., the XOR one. Let's see how \n",
    "we can model it.\n",
    "\n",
    "\n",
    "<pre>\n",
    "          W1                    __     h1\n",
    "X1o--------------------------  |   -----------o\n",
    "   \\                /        __|               \\\n",
    "    \\              /W2                          \\\n",
    "     \\            /                              \\ \n",
    "      \\          /                                \\\n",
    "       \\        /                                  \\\n",
    "        \\      /                                    \\w5\n",
    "         \\    /                                      \\\n",
    "          \\  /                                        \\\n",
    "            /                                          \\          __            \n",
    "           /\\                                          --------  |  -------o y\n",
    "          /  \\                                         /       __|              \n",
    "         /    \\                                       /\n",
    "        /      \\                                     /\n",
    "       /        \\                                   /w6\n",
    "      /          \\                                 /\n",
    "     /            \\                               /\n",
    "    /              \\W3                           /\n",
    "   /                \\                           /\n",
    "  /                  \\          __     h2      /\n",
    "X2o--------------------------  |   -----------o\n",
    "           W4                __|\n",
    "\n",
    "\n",
    "\n",
    "        X2 |                            h2  |\n",
    "           |                                |\n",
    "           1  0            ====>         ?  |  ?\n",
    "           |                                |\n",
    "   --------0--1----- X1             ---------------- h1\n",
    "           |                                |\n",
    "           |                             ?  |  ?\n",
    "           |                                |\n",
    "           |                                |\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def nn1HiddenLayer2(X,W,showHidden=None,debug=False):\n",
    "    assert len(W)==6 \n",
    "    strings=[False,False,False,False,False]\n",
    "    h=0\n",
    "    if showHidden : \n",
    "        h=(showHidden-1)%2 # h1-2\n",
    "        assert h>=0\n",
    "        strings[h]=True \n",
    "        \n",
    "    hl=[]\n",
    "    hl.append(nPerceptron(X=X,W=W[0:2],string=strings[0]) )\n",
    "    hl.append(nPerceptron(X=X,W=W[2:4],string=strings[1]) )\n",
    "\n",
    "    if showHidden: return hl[h]\n",
    "    \n",
    "    return nPerceptron(X=[hl[0],hl[1]],W=W[4:6],string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this is in effect a neural network: we have used the perceptron block and connected three of these together.\n",
    "\n",
    "If you run the scan function with the following weights, you will see that the points a(1,0), b(0,1) get mapped onto +1, while the points (0,0) and (1,1) get mapped onto -1. Thus, in effect this neural network can distinguish between them in the same way the XOR function does. It is therefore a faithful model of the XOR gate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1  0  1  \n",
      "\t1  1  1  -  \n",
      "\t0  1  -  1  \n",
      "\t-1 -  1  1  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "weights1Layer=[-1,1,\n",
    "              1,-1,\n",
    "              -1,-1\n",
    "             ]\n",
    "scan2InputNN(nn1HiddenLayer2,weights=weights1Layer,showHidden=None,xysup=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "<pre>\n",
    "XOR(o) = 1  |     No lime                                 No line \n",
    "XOR(x) = 0  o x   distinguish                        |    distinguish                  \n",
    "            |     o from x         W               o |    o from x\n",
    "      ______x_o____             ------->        _____x____\n",
    "            |                                        | o          \n",
    "            |                                        |             \n",
    "            |                \\                       | \n",
    "                              \\\n",
    "                               \\  S ° W \n",
    "            |                   \\                 \\  |       Infinite lines\n",
    "            | S                   \\                o | x     distinguish o from x\n",
    "            |                       \\------>   _____\\|_____  e.g., 2nd diagonal!\n",
    "            v                                        |\\\n",
    "                                                     | o\n",
    "                    No line                          |  \\\n",
    "            | ®     distinguish\n",
    "       _____|_____  o from x\n",
    "            |\n",
    "            |\n",
    "            \n",
    "      Neither the linear transformation W nor the non-linear step function S alone allow us \n",
    "      to distinguish the two input cases of the XOR function. However, combining both yields\n",
    "      a separable set.\n",
    "</pre>\n",
    "\n",
    "We can write the whole network as \n",
    "\n",
    "$y\\,=\\,S\\left( \\mathbf{\\overrightarrow{w'}}\\cdot \\mathbf{\\overrightarrow{h}} \\right)\\,=\\,S\\left((w_5,\\,w_6)\\cdot\\begin{pmatrix}h_1\\\\h_2\\end{pmatrix}\\right)$  \n",
    "$\\mathbf{\\overrightarrow{h}}\\,=\\,S\\left(\\mathbf{W}\\,\\mathbf{\\overrightarrow{x}}\\right)$\n",
    "\n",
    "$S\\big( (a,\\,b)\\big)\\,\\equiv\\,\\big((S(a),\\,S(b))\\big)$ and $S(a)\\,=\\,\\left\\{\\begin{array} \\,-1\\;\\mbox{if a<0}\\\\1\\;\\mbox{otherwise}\\end{array}\\right.$   \n",
    "<pre>\n",
    "        |         Properties of S:     \\  |  / Leaves invariant both diagonal\n",
    "       1|__ S(a)                        \\ | /            |       \n",
    "   _____|______                     _____\\|/______     x | +  Maps origin and positive base vec -> (1,1)\n",
    "      __|-1                              /|\\         __^_o_^__  \n",
    "        |                               / | \\            |›x  Maps negative base vec onto 2nd diagonal  \n",
    "        |                              /  |  \\           | \n",
    "            \n",
    "</pre>\n",
    "                                                 \n",
    "with function $S$ being the (non-linear, Heaviside) step function. Also,\n",
    "\n",
    "$\\mathbf{W}\\,\\mathbf{\\overrightarrow{x}}\\,=\\,\\begin{bmatrix}-1 & 1 \\\\1 & -1\\end{bmatrix}\\,\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix} $\n",
    "\n",
    "and \n",
    "\n",
    "$\\mathbf{\\overrightarrow{x}}\\,=\\,\\left\\{\\begin{array}\\,(1,0)^t \\\\ (0,1)^t\\end{array}\\right. \\quad\\Rightarrow\\quad W\\mathbf{\\overrightarrow{x}}\\,=\\,\\left\\{\\begin{array}\\,-(1,-1)^t\\\\-(-1,1)^t\\end{array}\\right.$\n",
    "\n",
    "$\\mathbf{\\overrightarrow{x}}\\,=\\,\\left\\{\\begin{array}\\,(1,1)^t \\\\ (0,0)^t\\end{array}\\right. \\quad\\Rightarrow\\quad W\\mathbf{\\overrightarrow{x}}\\,=\\,(0,0)$\n",
    "\n",
    "Thus, the linear transformation $\\mathbf{W}$ maps all four inputs **interspersed** along the second diagonal. On the other hand, if we'd feed the input directly to the non-linear step function $S$, all four input values would be mapped onto the single point $(1,1)$. Whence, in either case a projection onto any direction will not be able to untangle the two input cases of the XOR function.\n",
    "\n",
    "However, it works if we combine both! Let's see how. First $\\mathbf{W}$ maps the input cases onto the second diagonal and then the step function $S$ pulls the origin $(0,0$ off that diagonal and maps it onto point $(1,1)$. A straight line can then be used to distinguish them.\n",
    "\n",
    "\n",
    "**Question: Which line could we then use to actually classify the two input cases of the XOR function?** Ans: There is of course an infinite number of lines that do the job. Among them we have, e.g., the 2nd diagonal $y=-x$, but also any othe parallel to that cuts through the first quadrant but leaves point $(1,1)$ still on its right side, i.e., $y=-x+b$ with $0\\lt\\,b\\,\\lt 2$<sup id=\"ref2\">[2](#f2)</sup>.\n",
    "\n",
    "**Remark: Modelling the XOR function entails classifying a set of points that can *not* be classified by just drawing a straight, dividing line on the plane. We achieved that despite the fact the our basic building block, the perceptron, is only capable of making a linear classification, i.e., based on a line!**.\n",
    "\n",
    "\n",
    "<sup id=\"f2\">[2]</sup>Usually in the literature the perceptron is defined including a constant terms $b$. In the Machine learning community this is known as the bias; in physics, however, it's called the external field. Here, we haven't mentioned so far such a term. We will see later how it can be introduced in a natural way by simply building on the freedom we have for setting up learning/classifying machines.[^](#ref2)\n",
    "\n",
    "\n",
    "<!--\n",
    "#### Geometrical interpretation\n",
    "The linear transformation given by $\\mathbf{W}$ can be seen as $\\mathbb{-1}\\,+\\,\\mathbf{\\sigma_x}$, where $\\sigma_x\\,=\\,\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\,=\\,R_{-\\pi/2}\\cdot m_y$, i.e., a reflection over the y-axis $m_y$ (inverting the orientation of the x-axis) followed by a clock-wise rotation of $90\\deg$, $R_{-\\pi/2}$. Furthermore, the first term of $\\mathbf{W}$, which corresponds to an inversion over the origin, can be constructed as a reflection over $y$-axis followed by another over the $x$-axis, i.e., $\\mathbb{-1}\\,=\\,m_x\\,m_y$. Whence, it is\n",
    "$$\\mathbf{W}\\,=\\,\\left[m_x\\,+\\,R_{-\\pi/2}\\right]\\,m_y$$\n",
    "\n",
    "This leads to  the mapping $\\overrightarrow{x}\\,\\to\\,-\\overrightarrow{x}\\,+\\,\\overrightarrow{x'}$ where $\\overrightarrow{x'}\\,=\\,(x_2,x_1)$. In terms of the reflections, $\\overrightarrow{x}\\,\\to\\,-\\,(1\\,-\\,R_{-\\pi/2}\\,m_y)\\,\\overrightarrow{x}$\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron, aka., NN\n",
    "\n",
    "Some more examples of non-linear classificationsc\n",
    "\n",
    "<pre>\n",
    "          W1                           __     h1\n",
    "X1o---------------------------------  |   ------------o\n",
    "   \\ \\              /               __|                \\\n",
    "    \\  \\           /W2                                  \\\n",
    "     \\   \\        /                                      \\ \n",
    "      \\    \\     /                                        \\\n",
    "       \\     \\  /                                          \\\n",
    "        \\      /\\                                           \\w7\n",
    "         \\    /  \\ W3                                        \\\n",
    "          \\  /    \\                                           \\\n",
    "            /      \\                    __     h2          w8  \\           __            \n",
    "           /\\       /----------------  |   ------------o----------------  |   --------o y\n",
    "          /  \\     /                 __|                       /        __|              \n",
    "         /    \\   /W4                                         /\n",
    "        /      \\ /                                           /\n",
    "       /      / \\                                           /w9\n",
    "      /     /    \\                                         /\n",
    "     /    /       \\                                       /\n",
    "    /   /          \\W5                                   /\n",
    "   /  /             \\                                   /\n",
    "  / /                \\                  __     h3      /\n",
    "X2o----------------------------------  |   ------------o\n",
    "           W6                        __|\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "\n",
    "def nn1HiddenLayer3(X,W,showHidden=None,debug=False):\n",
    "    z1 = X[0]*W[0]+X[1]*W[1];\n",
    "    z2 = X[0]*W[2]+X[1]*W[3];\n",
    "    z3 = X[0]*W[4]+X[1]*W[5];\n",
    "\n",
    "    h1 = -1 if z1<0 else 1;\n",
    "    h2 = -1 if z2<0 else 1;\n",
    "    h3 = -1 if z3<0 else 1;\n",
    "  \n",
    "    z  = h1*W[6]+h2*W[7]+h3*W[8];\n",
    "    return \"-\" if z<0 else \"1\";\n",
    "    \n",
    "def ng_nn1HiddenLayer3(X,W,string=False,debug=False,activation=nonLinearBipolarStep,showHidden=None):\n",
    "    h1 = perceptron(X[0],X[1],W[0],W[1],string=False,debug=debug,activation=activation)\n",
    "    h2 = perceptron(X[0],X[1],W[2],W[3],string=False,debug=debug,activation=activation)\n",
    "    h3 = perceptron(X[0],X[1],W[4],W[5],string=False,debug=debug,activation=activation)\n",
    "    \n",
    "    return nPerceptron([h1,h2,h3],[W[6],W[7],W[8]],string=str,debug=debug,activation=activation) ;\n",
    "\n",
    "weights1HiddenLayer3 = [0,1,\n",
    "     1,1,\n",
    "     2,3,\n",
    "     1,-2,1]\n",
    "\n",
    "len(weights1HiddenLayer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t0  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-1 1  1  1  1  1  1  -  -  -  -  -  \n",
      "\t-2 1  1  1  1  1  1  1  -  -  -  -  \n",
      "\t-3 1  1  1  1  1  1  1  1  -  -  -  \n",
      "\t-4 1  1  1  1  1  1  1  1  1  -  -  \n",
      "\t-5 1  1  1  1  1  1  1  1  1  1  -  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(ng_nn1HiddenLayer3,weights1HiddenLayer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<pre>\n",
    "          W1                           __     h1\n",
    "X1o---------------------------------  |   ------------o\n",
    "   \\ \\              /               __|                \\\n",
    "    \\  \\           /W2                                  \\\n",
    "     \\   \\        /                                      \\ \n",
    "      \\    \\     /                                        \\\n",
    "       \\     \\  /                                          \\\n",
    "        \\      /\\                                           \\w7\n",
    "         \\    /  \\ W3                                        \\\n",
    "          \\  /    \\                                           \\\n",
    "            /      \\                    __     h2          w8  \\           __            \n",
    "           /\\       /----------------  |   ------------o----------------  |   --------o y\n",
    "          /  \\     /                 __|                       /        __|              \n",
    "         /    \\   /W4                                         /\n",
    "        /      \\ /                                           /\n",
    "       /      / \\                                           /w9\n",
    "      /     /    \\                                         /\n",
    "     /    /       \\                                       /\n",
    "    /   /          \\W5                                   /\n",
    "   /  /             \\                                   /\n",
    "  / /                \\                  __     h3      /\n",
    "X2o----------------------------------  |   ------------o\n",
    "           W6                        __|\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -10  -9  -8  -7  -6  -5  -4  -3  -2  -1  0  1  2  3  4  5  6  7  8  9  10  \n",
      "\t10  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t9  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t8  -  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t7  -  -  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t6  1  -  -  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t5  1  1  1  -  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  -  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  1  1  1  1  1  1  -  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  1  1  1  1  1  1  1  -  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-1 1  1  1  1  1  1  1  1  1  1  1  1  -  -  -  -  -  -  -  -  -  \n",
      "\t-2 1  1  1  1  1  1  1  1  1  1  1  1  1  -  -  -  -  -  -  -  -  \n",
      "\t-3 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  -  -  -  -  -  -  \n",
      "\t-4 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  -  -  -  -  -  \n",
      "\t-5 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  -  -  -  \n",
      "\t-6 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  -  -  \n",
      "\t-7 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-8 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-9 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-10 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "def nn1HiddenLayer3B(X,W,showHidden=None,debug=False):\n",
    "    \"\"\"\n",
    "    The same NN with 1 hidden layer of 3 neurons.\n",
    "    \n",
    "    Rewritten here for implementing visualizing each internal neuron separately.\n",
    "    \"\"\"\n",
    "    strings=[False,False,False]\n",
    "    h=0\n",
    "    if showHidden : \n",
    "        h=showHidden-1\n",
    "        assert h>=0\n",
    "        strings[h]=True \n",
    "        \n",
    "    hl=[]\n",
    "    hl.append( perceptron(X[0],X[1],W[0],W[1],strings[0]) )\n",
    "    hl.append( perceptron(X[0],X[1],W[2],W[3],strings[1]) )\n",
    "    hl.append( perceptron(X[0],X[1],W[4],W[5],strings[2]) )\n",
    "    \n",
    "    if debug: print(\"h1: \",hl[0],\"  h2: \",hl[1],\"  h3: \",hl[2],\"\\nh: \",h,\"  showHidden: \",showHidden)\n",
    "        \n",
    "    if showHidden: return hl[h]\n",
    "        \n",
    "    z = hl[0]*W[6]+hl[1]*W[7]\n",
    "    \n",
    "    if debug: print(\"S^{-1}(y): \",z+hl[2]*W[8],\"\\nW7 / W8 / W9 : \",W[6],W[7],W[8])\n",
    "        \n",
    "    return perceptron(z,hl[2],1,W[8],string=True)\n",
    "\n",
    "weights1HiddenLayer3B = [0,1,\n",
    "     1,1,\n",
    "     2,3,\n",
    "     1,2,-3]\n",
    "\n",
    "scan2InputNN(ng_nn1HiddenLayer3,weights1HiddenLayer3B,xysup=21,showHidden=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1:  1   h2:  1   h3:  1 \n",
      "h:  0   showHidden:  None\n",
      "S^{-1}(y):  0 \n",
      "W7 / W8 / W9 :  1 -2 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1HiddenLayer3B([3,2],weights1HiddenLayer3,showHidden=None,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Layers NN\n",
    "<pre>\n",
    "          W1                           __     h1\n",
    "X1o---------------------------------  |   ------o\n",
    "   \\ \\              /               __|          \\\n",
    "    \\  \\           /W2                            \\w7\n",
    "     \\   \\        /                                \\ \n",
    "      \\    \\     /                                  \\            __       J1\n",
    "       \\     \\  /                                    /--------  |   ------o\n",
    "        \\      /\\                                   /         __|          \\\n",
    "         \\    /  \\ W3                              /w8                      \\w11\n",
    "          \\  /    \\                               /                          \\\n",
    "            /      \\                    __     h2/                            \\        __            \n",
    "           /\\       /----------------  |   -----o  ----------/     /----------------  |   ------------o y\n",
    "          /  \\     /                 __|         \\                             /    __|              \n",
    "         /    \\   /W4                             \\w9                         /\n",
    "        /      \\ /                                 \\                         /w12\n",
    "       /      / \\                                   \\             __        /\n",
    "      /     /    \\                                   /---------  |   ------o\n",
    "     /    /       \\                                 /          __|         J2\n",
    "    /   /          \\W5                             /w10       \n",
    "   /  /             \\                             /      \n",
    "  / /                \\                  __     h3/      \n",
    "X2o----------------------------------  |   -----o\n",
    "           W6                        __|\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn2HiddenLayer32(X,W,showHidden=None,debug=False):\n",
    "    assert len(W)==12 \n",
    "    strings=[False,False,False,False,False]\n",
    "    h=0\n",
    "    if showHidden : \n",
    "        h=showHidden-1 # h1-3: 0-2 ; j1-2:3,4\n",
    "        assert h>=0 \n",
    "        strings[h]=True \n",
    "        \n",
    "    hl=[]\n",
    "    hl.append(nPerceptron(X=X,W=W[0:2],string=strings[0]) )\n",
    "    hl.append(nPerceptron(X=X,W=W[2:4],string=strings[1]) )\n",
    "    hl.append(nPerceptron(X=X,W=W[4:6],string=strings[2]) )\n",
    "    \n",
    "    if showHidden and h<3: return hl[h]\n",
    "    \n",
    "    jl=[]\n",
    "    jl.append(nPerceptron(X=[hl[0],hl[1]],W=W[6:8],string=strings[3]) )\n",
    "    jl.append(nPerceptron(X=[hl[1],hl[2]],W=W[8:10],string=strings[4]) )\n",
    "\n",
    "    if showHidden: return jl[h-3]  #only happens if not printing first hidden layer hl[h]\n",
    "\n",
    "    if debug:\n",
    "        for i in range(5):\n",
    "            if i<3:\n",
    "                v = hl[i]\n",
    "                idx = i\n",
    "                lbl='h'\n",
    "            else:\n",
    "                v = jl[i-3]\n",
    "                idx = i-3\n",
    "                lbl='J'\n",
    "            print(lbl,\"\",idx,\": \",v,\"  \",end=\"\",sep=\"\")\n",
    "        print(\"\\nh: \",h,\"  showHidden: \",showHidden,sep=\"\")\n",
    "\n",
    "    if showHidden: return hl[h] if h<3 else jl[h-3]\n",
    "    \n",
    "    return nPerceptron(X=[jl[0],jl[1]],W=W[10:12],string=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  -  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  -  -  1  1  1  1  1  1  1  1  1  \n",
      "\t1  -  -  -  -  1  1  1  1  1  1  1  \n",
      "\t0  -  -  -  -  -  1  1  1  1  1  1  \n",
      "\t-1 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-2 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-3 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-4 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-5 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "weights2Layers=[0,1,       #w1,w2\n",
    "                1,1,       #w3,w4\n",
    "                2,3,       #w5,w6\n",
    "                -0.5,-.2,-.1,0.5, #w7,w8,w9,w10\n",
    "                1,1]       #w11,12\n",
    "\n",
    "#len([1,23,3])\n",
    "#len(weights2Layers)\n",
    "#nn2HiddenLayer32([3,3],weights2Layers)\n",
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=None,xysup=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How?\n",
    "\n",
    "Let's see how each hidden layer contributes to achieve the above classification of the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing hidden neuron  1\n",
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t1  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t0  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-1 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-2 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-3 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-4 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-5 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=1,xysup=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing hidden neuron  2\n",
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  -  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  -  -  1  1  1  1  1  1  1  1  1  \n",
      "\t2  -  -  -  1  1  1  1  1  1  1  1  \n",
      "\t1  -  -  -  -  1  1  1  1  1  1  1  \n",
      "\t0  -  -  -  -  -  1  1  1  1  1  1  \n",
      "\t-1 -  -  -  -  -  -  1  1  1  1  1  \n",
      "\t-2 -  -  -  -  -  -  -  1  1  1  1  \n",
      "\t-3 -  -  -  -  -  -  -  -  1  1  1  \n",
      "\t-4 -  -  -  -  -  -  -  -  -  1  1  \n",
      "\t-5 -  -  -  -  -  -  -  -  -  -  1  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=2,xysup=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing hidden neuron  3\n",
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  -  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  -  -  1  1  1  1  1  1  1  1  1  \n",
      "\t1  -  -  -  -  1  1  1  1  1  1  1  \n",
      "\t0  -  -  -  -  -  1  1  1  1  1  1  \n",
      "\t-1 -  -  -  -  -  -  -  1  1  1  1  \n",
      "\t-2 -  -  -  -  -  -  -  -  1  1  1  \n",
      "\t-3 -  -  -  -  -  -  -  -  -  -  1  \n",
      "\t-4 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-5 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=3,xysup=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing hidden neuron  4\n",
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t4  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t3  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t2  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t1  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t0  -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-1 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-2 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-3 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-4 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t-5 1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=4,xysup=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing hidden neuron  5\n",
      "      -5  -4  -3  -2  -1  0  1  2  3  4  5  \n",
      "\t5  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t4  1  1  1  1  1  1  1  1  1  1  1  \n",
      "\t3  -  1  1  1  1  1  1  1  1  1  1  \n",
      "\t2  -  -  1  1  1  1  1  1  1  1  1  \n",
      "\t1  -  -  -  -  1  1  1  1  1  1  1  \n",
      "\t0  -  -  -  -  -  1  1  1  1  1  1  \n",
      "\t-1 -  -  -  -  -  -  -  1  1  1  1  \n",
      "\t-2 -  -  -  -  -  -  -  -  1  1  1  \n",
      "\t-3 -  -  -  -  -  -  -  -  -  -  1  \n",
      "\t-4 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t-5 -  -  -  -  -  -  -  -  -  -  -  \n",
      "\t"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nn2HiddenLayer32,weights=weights2Layers,showHidden=5,xysup=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generalizing the Perceptron\n",
    "\n",
    "We have seen that we can understand the classical perceptron as a 2-step process, (1) taking an average and (2) labeling that average among two classes (the nonlinear step).\n",
    "\n",
    "We also claimed that any nonlinearity will help us do the job. We want to see now that also any \"aggregator\" function will do.\n",
    "<pre>\n",
    "   X1 o  ___       __\n",
    "       \\(   )___  |  ____o \n",
    "       /(___)   __|      \n",
    "   X2 o\n",
    "   \n",
    "The general aggregator function, also called predictor function. This simply takes N inputs (here shown only two)\n",
    "and outputs a value. The latter may not necessarily be neither linear, nor continous nor even a deterministic \n",
    "function of the N inputs X1,...XN. This value is then eventually feeded into the non-linear activation function.\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synapticPotentialWeighted(X,W):\n",
    "    return np.dot(X,W[:len(X)])\n",
    "    \n",
    "def synapticPotentialAverage(X,W=None):\n",
    "    return sum(X)/len(X)\n",
    "    \n",
    "def synapticPotentialMedian(X,W=None):\n",
    "    \"\"\"\n",
    "    To make sense, \n",
    "    WT := \\sum |W_i| := total # of X values (not all distinct) seen.\n",
    "    Assing each X_i to its weight W_i -> (X_i,W_i)\n",
    "    sort by X_i\n",
    "    lx2=floor(WT/2)\n",
    "    if exists j ; |W_j| = lx2 :\n",
    "        md = ( W_j*X_j + W_(j+1) *X_(j+1) ) / ( W_j + W_(j+1)) #weighted average of both points\n",
    "    if WT%2==0: \n",
    "    else:\n",
    "        md\n",
    "        \n",
    "    Example: w1=7 w2=5 w3=2 => WT=14 => lx2=7 => md = (w1*x1+w2*x2 ) / (w1+w2) = (7x1+5x2)/12\n",
    "                  \n",
    "    if lx%2 == 0 ):\n",
    "        md=(Z[lx2-1] + Z[lx2] )*0.5\n",
    "    else:\n",
    "        md=Z[lx2]*1.0 #so that it always returns a float\n",
    "    \"\"\"\n",
    "    \n",
    "    # W=[1,1,...,1] as many 1's as length of X\n",
    "    if W == None: \n",
    "        W=np.ones(len(X)) \n",
    "\n",
    "    #Given array [x1,x2,...xn] -> get the sum_i xi\n",
    "    def foldr(_X,_accu=0):          \n",
    "        if len(_X) == 0: return _accu\n",
    "        if len(_X) == 1: return _accu+_X[0]\n",
    "        return (_accu + _X[-1] + foldr(_X[:-1]) )\n",
    "\n",
    "    WT = foldr( list( map(abs,W) ) )\n",
    "    \n",
    "    def pfilter(par,x):\n",
    "        for a,b in par:\n",
    "            if abs(b) == x: return par.index((a,b))\n",
    "        return None\n",
    "\n",
    "    Z=sorted(list(zip(X,W)))\n",
    "    lx2=floor(WT/2)\n",
    "    #on Z=sorted(X,W): Python will mutate arguments **if it can**\n",
    "               \n",
    "    #if WT%2 == 0:\n",
    "    #    j = pfilter(Z,lx2)\n",
    "    #    try: return (Z[j][1]*Z[j][0]+Z[j+1][1]*Z[j+1][0]) \n",
    "    #    except: print('ERROR j= ',j,\" Z: \",Z, \" WT: \",WT)\n",
    "    #else:\n",
    "    j=0 ; zw=abs(Z[j][1])\n",
    "    while zw < lx2:\n",
    "        j+=1\n",
    "        zw+=abs(Z[j][1]) \n",
    "    if j == len(Z)-1: j-=1\n",
    "    \n",
    "    try: return (Z[j+1][1]*Z[j+1][0]+Z[j][1]*Z[j][0])/(abs(Z[j+1][1])+abs(Z[j][1])) # {wj*xj + w(j+1)*x(j+1)}/(wj+wj+1) \n",
    "    except: print('ERROR j=',j,\" Z: \",Z, \" WT: \",WT)\n",
    "\n",
    "def synapticPotentialMode(X,W):\n",
    "    \"\"\"\n",
    "        To make sense, the weights W should be non-negative.\n",
    "    Choose randomly one among all possible maxima\n",
    "    \n",
    "    When W[1,1] Provides a soft boundary between both phases (classes) with difussion across the boundary\n",
    "    \"\"\"\n",
    "    wmax = max(list(map(abs,W)) )\n",
    "    imax = [ i for i,j in enumerate(list(map(abs,W)) ) if j==wmax ]\n",
    "    #print(\"max weights:\",imax)\n",
    "    i = np.random.choice(imax,1)\n",
    "    return X[i]*W[i]/wmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 7], -4.998800119988001, 12, 6.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:2],synapticPotentialMedian(b[:2],[-10000,1]),synapticPotentialWeighted(b[:2],[1,1]),synapticPotentialAverage(b[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 7], -4.998800119988001, 12, 6.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:2],synapticPotentialMedian(b[:2],[-10000,1]),synapticPotentialWeighted(b[:2],[1,1]),synapticPotentialAverage(b[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 7], -4.998800119988001, 12, 6.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:2],synapticPotentialMedian(b[:2],[-10000,1]),synapticPotentialWeighted(b[:2],[1,1]),synapticPotentialAverage(b[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 7], -4.998800119988001, 12, 6.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:2],synapticPotentialMedian(b[:2],[-10000,1]),synapticPotentialWeighted(b[:2],[1,1]),synapticPotentialAverage(b[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nGenPerceptron(X,W,\n",
    "                   synapticPotential=synapticPotentialMode,\n",
    "                   activation=nonLinearBipolarStep,\n",
    "                   #activation=(lambda z,s: nonLinearBipolarStep(nonLinearRelu(z,None),s)),\n",
    "                   string=True,debug=False,showHidden=False):\n",
    "    assert len(X)>1 , len(W)>1 \n",
    "    \n",
    "    return activation(synapticPotential(X,W),string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1  0  1  \n",
      "\t1  -  -  -  \n",
      "\t0  1  1  1  \n",
      "\t-1 1  1  1  \n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:74: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "scan2InputNN(nGenPerceptron,weights=[0,-1],xysup=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias or \"external field\"\n",
    "\n",
    "This is a more detail model of a neuron\n",
    "![detailed diagram neuron wikipedia](https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg)\n",
    "\n",
    "Connections between a neuron A and another B are established from the axon terminals of A to the dendrites of B (see on the left orange branches and red ones). It seems reasonable to think that the neuron is capable of \"doing\" something else with all the inputs it gets than just \"summing that up\".\n",
    "\n",
    "We could think of enlarging our simple perceptron model by giving it a kind of \"cell body\".\n",
    "\n",
    "Let's say the neuron doesn't just weight its inputs but diverts part of them to each other. In the following drawing, the weighted input from neuron 2 is split into two parts and one of them is added to the weighted contribution of neuron 1. Finally, a layer of two hidden neurons re-weights those two braches and the neuron decides whether to fire or not (or say to fire either +1 or -1) down its axon to the connection it's making with other neurons.\n",
    "\n",
    "<pre>\n",
    "\n",
    "X1 o\n",
    "    \\z1  x1*z1+&#937;\n",
    "     \\ _________            <!-- due to unicode char, drawing _seems_ misaligned. However renders well -->\n",
    "      |         \\z3          _\n",
    "     &#937;|          ----------_| ------o y\n",
    "      |_________/z4   \n",
    "     /  x2*z2-&#937;\n",
    "    /z2 \n",
    "X2 o\n",
    "</pre>\n",
    "\n",
    "Let's call this new perceptron the **L-perceptron**, from the loop circuit this enlargement represents. \n",
    "\n",
    "We have then the the U-perceptron mixes up part of the inputs before thresholding. In this case, it diverts an amount of $\\Omega$ from one input to the other. On the upper branch we then have $z_1\\cdot x_1\\,+\\,\\Omega$ while the lower branch carries $z_2\\cdot x_2\\,-\\,\\Omega$. Finally, the U-perceptron will fire a value $y$ depending on the aggregation signal $z\\,\\equiv\\,z_3\\cdot z_1\\cdot x_1\\,+\\,+\\,z_4\\cdot z_2\\cdot x_2\\,+\\,\\Omega\\,(z_3\\,-\\,z_4)$.\n",
    "\n",
    "Let's introduce the following variables\n",
    "$$w_1\\,=\\,z_3\\,z_1 \\\\\n",
    "  w_2\\,=\\,z_4\\,z_2 \\\\\n",
    "  b  \\,=\\,\\Omega\\,(z3-z4)\n",
    "$$\n",
    "\n",
    "Then the U-perceptron aggregator amounts to $$z\\,=\\,w_1\\, x_1\\,+\\,w_2\\, x_2\\,+\\,b\\,$$\n",
    "\n",
    "We see that the L-perceptron is simply our previous perceptron with an added *bias*, $b$ !! \n",
    "\n",
    "In the physics community, the bias $b$ is called the external field. In the context of modeling magnetic materials, the external field could be the external magnetic field acting on the sample material. \n",
    "\n",
    "From what we have seen so far, it is easy to realize that the bias introduces an additional source of signal before the neuron decides to fire +1 or -1. The L-perceptron is no longer a passive element, but actively contributes to the incoming signal. \n",
    "\n",
    "More specifically, the bias allows us to draw dividing lines that do not cross the origin (0,0) in a simple way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Hull\n",
    "\n",
    "From this point on we will asume our perceptron is an L-perceptron, i.e., it comes with a bias $b$.\n",
    "\n",
    "Let's see how we can use the perceptron to classify a convex region in the plane.\n",
    "\n",
    "<pre>\n",
    "         |\n",
    "      ___|1___\n",
    " ____|___|____|______\n",
    "   -2|___|____|2\n",
    "       -1|\n",
    "         |\n",
    "      \n",
    "\n",
    "</pre>\n",
    "\n",
    "We can geneate four classifying lines with perpendiculars and biases given by L1: (0,1),1  L2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 36, 9, 49, 4], 99, 19)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=map((lambda x: x*x),a)\n",
    "z= list(z)\n",
    "\n",
    "def foldr(_X,_accu=0):\n",
    "    \"\"\"\n",
    "    foldR\n",
    "    \"\"\"\n",
    "    if len(_X) == 0: return _accu\n",
    "    if len(_X) == 1: return _accu+_X[0]\n",
    "    return (_accu + _X[-1] + foldr(_X[:-1]) )\n",
    "\n",
    "\n",
    "\n",
    "list(z),foldr(z),foldr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -2, 3]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,-2,3]\n",
    "c=list(map(abs,a))\n",
    "np.dot(a,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 36, 9, 49, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 1), (6, 36), (3, 9), (7, 49), (2, 4)],\n",
       " [(1, 1), (2, 4), (3, 9), (6, 36), (7, 49)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=list(zip(a,z))\n",
    "p,sorted(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "def pfilter(par,x):\n",
    "    for a,b in par:\n",
    "        if b == x: return par.index((a,b))\n",
    "    return None\n",
    "\n",
    "j=pfilter(p,490)\n",
    "if j: print(j)\n",
    "else: print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 49)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
